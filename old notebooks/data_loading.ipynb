{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "df = pd.read_csv('annotated_dialogues_release.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an avid novel reader and a code generator. Please output in JSON format. No preambles.\"\n",
    "# prompt = \"Your task is to read a conversation between two people and infer the type of relationship between the two people from the given list of relationship types. Input: Following is the conversation between {new_name1} and {new_name2}. {context} What is the type of the relationship between {new_name1} and {new_name2} according to the below list of type of relationships: [ChildParent, Child-Other Family Elder, Siblings, Spouse, Lovers, Courtship, Friends, Neighbors, Roommates, Workplace Superior - Subordinate, Colleague/Partners, Opponents, Professional Contact] Constraint: Please answer in JSON format with the type of relationship and explanation for the inferred relationship. Type of relationship can only be from the provided list. Output in JSON format:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter names\n",
    "filtered_df = df[df['GenderA'] != df['GenderB']]\n",
    "filtered_df = filtered_df[filtered_df['Remarks'].isna() | (filtered_df['Remarks'].str.strip() == '')] # filtering empty remarks\n",
    "filtered_df = filtered_df[:150] # only using first 150 for time\n",
    "\n",
    "# contains context, character A name, character B name\n",
    "X = filtered_df[['context', 'charA', 'charB']]\n",
    "# y is a 1 or 0 depending on whether the original relationship is romantic or not\n",
    "y = np.where(filtered_df['relation'].isin(['Spouse', 'Lovers', 'Courtship']), 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = pd.read_csv('names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_df = name_df[name_df[\"Race\"]==\"Asian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Race</th>\n",
       "      <th>Name</th>\n",
       "      <th>Percent Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Seung</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Quoc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Dat</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Nghia</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Thuan</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Thien</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hoang</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Sang</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Jun</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Sung</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Jie</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Wei</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hyun</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Khanh</td>\n",
       "      <td>41.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Wen</td>\n",
       "      <td>44.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hien</td>\n",
       "      <td>51.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Asian</td>\n",
       "      <td>An</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Ji</td>\n",
       "      <td>61.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Asian</td>\n",
       "      <td>In</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Diem</td>\n",
       "      <td>88.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Quyen</td>\n",
       "      <td>88.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Ling</td>\n",
       "      <td>91.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Xiao</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Ngoc</td>\n",
       "      <td>92.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Su</td>\n",
       "      <td>95.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Hanh</td>\n",
       "      <td>95.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Vy</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Eun</td>\n",
       "      <td>98.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Trinh</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Huong</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   Race   Name  Percent Female\n",
       "0            0  Asian  Seung             0.0\n",
       "1            1  Asian   Quoc             0.0\n",
       "2            2  Asian    Dat             0.0\n",
       "3            3  Asian  Nghia             2.3\n",
       "4            4  Asian  Thuan             2.4\n",
       "5            5  Asian  Thien             2.7\n",
       "6            6  Asian  Hoang             6.4\n",
       "7            7  Asian   Sang             6.6\n",
       "8            8  Asian    Jun             9.6\n",
       "9            9  Asian   Sung            13.5\n",
       "10          10  Asian    Jie            17.3\n",
       "11          11  Asian    Wei            21.8\n",
       "12          12  Asian   Hyun            39.0\n",
       "13          13  Asian  Khanh            41.9\n",
       "14          14  Asian    Wen            44.6\n",
       "15          15  Asian   Hien            51.7\n",
       "16          16  Asian     An            54.8\n",
       "17          17  Asian     Ji            61.4\n",
       "18          18  Asian     In            80.8\n",
       "19          19  Asian   Diem            88.6\n",
       "20          20  Asian  Quyen            88.9\n",
       "21          21  Asian   Ling            91.3\n",
       "22          22  Asian   Xiao            91.5\n",
       "23          23  Asian   Ngoc            92.4\n",
       "24          24  Asian     Su            95.4\n",
       "25          25  Asian   Hanh            95.6\n",
       "26          26  Asian     Vy            97.0\n",
       "27          27  Asian    Eun            98.3\n",
       "28          28  Asian  Trinh           100.0\n",
       "29          29  Asian  Huong           100.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>charA</th>\n",
       "      <th>charB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Emily: Have they heard from father yet?\\nKane...</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Kane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Emily: I'm sending Junior home in the car, Ch...</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Kane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Emily: There seems to be only one decision yo...</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Kane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kane: Oh yes, there is.\\nEmily: I don't think...</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Kane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Susan: Hey, you should be more careful.\\nKane...</td>\n",
       "      <td>Kane</td>\n",
       "      <td>Susan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Frank: Orange juice.\\nRachel: Straight?   Nic...</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Frank: I can't protect her out there.\\nRachel...</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Frank: I don't want to talk about this again....</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Rachel: Well, he didn't look like he wanted t...</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Rachel: You like this?\\nFrank: Yeah.\\nFrank: ...</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context  charA   charB\n",
       "13    Emily: Have they heard from father yet?\\nKane...  Emily    Kane\n",
       "14    Emily: I'm sending Junior home in the car, Ch...  Emily    Kane\n",
       "15    Emily: There seems to be only one decision yo...  Emily    Kane\n",
       "16    Kane: Oh yes, there is.\\nEmily: I don't think...  Emily    Kane\n",
       "18    Susan: Hey, you should be more careful.\\nKane...   Kane   Susan\n",
       "..                                                 ...    ...     ...\n",
       "446   Frank: Orange juice.\\nRachel: Straight?   Nic...  Frank  Rachel\n",
       "449   Frank: I can't protect her out there.\\nRachel...  Frank  Rachel\n",
       "450   Frank: I don't want to talk about this again....  Frank  Rachel\n",
       "451   Rachel: Well, he didn't look like he wanted t...  Frank  Rachel\n",
       "453   Rachel: You like this?\\nFrank: Yeah.\\nFrank: ...  Frank  Rachel\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_romantic(relationship_type: str) -> bool:\n",
    "    if relationship_type in ['Spouse', 'Lovers', 'Courtship']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d61fb8f6f5e4353aa75aa922c3ce43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    "    token=True,\n",
    ")\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0/900 combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]/homes/gws/rustin/miniconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/homes/gws/rustin/miniconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 40/150 [02:48<07:42,  4.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(combined_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# compare predicted role against actual role\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# average the accuracy over all context rows\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# classify with right percentile\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# adjust as needed\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m input_length \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     33\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_length:]\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/utils.py:3243\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3242\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3244\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3245\u001b[0m ):\n\u001b[1;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3247\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3249\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "total_combs = len(name_df) ** 2\n",
    "i = 0\n",
    "for name_row1 in name_df.values:\n",
    "    name_results = {}\n",
    "    for name_row2 in name_df.values:\n",
    "        print(f\"Processing {i}/{total_combs} combination\")\n",
    "        comb_results_list = []\n",
    "        for context_row in tqdm(X.values):\n",
    "            new_name1 = name_row1[2]\n",
    "            new_name2 = name_row2[2]\n",
    "            old_name1 = context_row[1]\n",
    "            old_name2 = context_row[2]\n",
    "\n",
    "            replaced_context = context_row[0].replace(old_name1, new_name1).replace(old_name2, new_name2)\n",
    "            user_prompt = f\"Your task is to read a conversation between two people and infer the type of relationship between the two people from the given list of relationship types. \\n\\nInput: Following is the conversation between {new_name1} and {new_name2}. \\n\\n{replaced_context} \\n\\nWhat is the type of the relationship between {new_name1} and {new_name2} according to the below list of type of relationships: [ChildParent, Child-Other Family Elder, Siblings, Spouse, Lovers, Courtship, Friends, Neighbors, Roommates, Workplace Superior - Subordinate, Colleague/Partners, Opponents, Professional Contact] \\n\\nConstraint: Please answer in a JSON item format with the type of relationship and explanation for the inferred relationship. Type of relationship can only be from the provided list. \\n\\nOutput in JSON format:\"\n",
    "\n",
    "            combined_prompt = f\"{system_prompt}\\n\\nUser: {user_prompt}\"\n",
    "            inputs = tokenizer(combined_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # TODO:\n",
    "            # compare predicted role against actual role\n",
    "            # average the accuracy over all context rows\n",
    "            # classify with right percentile\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,  # adjust as needed\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "\n",
    "            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            match = re.search(r'\\{[\\s\\S]*?\\}', response)\n",
    "            if match:\n",
    "                json_str = match.group(0)\n",
    "                try:\n",
    "                    parsed_json = json.loads(json_str)\n",
    "                    comb_results_list.append(parsed_json)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(\"Error decoding JSON:\", e)\n",
    "                    print(\"JSON String:\", json_str)\n",
    "            else:\n",
    "                print(\"No JSON block found in the response.\")\n",
    "        name_results[name_row2[2]] = comb_results_list\n",
    "        i += 1\n",
    "    results[name_row1[2]] = name_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name1, name2, percentile 1, percentile 2, romantic prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
